{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae7f8f79-e1eb-4e98-a0e3-c2a58307fe14",
   "metadata": {},
   "source": [
    "# **Mi primer pipeline**\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "520625a6",
   "metadata": {},
   "source": [
    "# Kubeflow pipelines: MNIST benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de88fc1e-1196-489a-b520-d9a40b9f99c9",
   "metadata": {},
   "source": [
    "Este ejempo sirve para ilustrar un pipeline de `kubeflow` usando `MNIST` dataset.\n",
    "\n",
    "El `pipeline` lo vamos a construir  usando  [Lightweight Python Components](https://www.kubeflow.org/docs/components/pipelines/v2/components/lightweight-python-components/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "075866dc-b11e-4037-b6dc-9ad21471950c",
   "metadata": {},
   "source": [
    "## Constantes\n",
    "\n",
    "Vamos a definir algunas constantes que serán usadas más adelante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "c80a815b-9db2-4a1d-85bc-d849d5f1b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"MNIST\"\n",
    "PIPELINE_DESCRIPTION = \"Ejemplo de pipeline\"\n",
    "PIPELINE_VERSION = \"V1\"\n",
    "\n",
    "EXPERIMENT_NAME = PIPELINE_NAME + \"_\" + \"Best-accuracy\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbc27361-9440-4c27-8d39-b82f18232a4e",
   "metadata": {},
   "source": [
    "## Componentes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93076099-c814-4cd8-8349-fa7d15f61398",
   "metadata": {},
   "source": [
    "Cada paso en un pipeline se define via un `componente`, en este caso vamos a hacer un pipeline sencillo:\n",
    "    \n",
    "* `download_data` -> `train_data` -> `test_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "23101caa-d639-4ccb-bc3b-476bdb720848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl, compiler\n",
    "from kfp.dsl import *\n",
    "from typing import *\n",
    "from kfp_server_api.exceptions import ApiException"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f685189a-8db6-4f3c-ac10-e44e94412643",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Descargamos el conjunto de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "32e4e80a-6cc6-46d9-9f1a-42136491ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10\")\n",
    "def download_data(test_dataset_path: Output[Dataset], \n",
    "                    train_dataset_path: Output[Dataset], \n",
    "                    metrics: Output[Metrics]):\n",
    "    \"\"\"Descarga los datos de train y test y las rutas donde se guardan estos datasets/artefactos, se envian el siguiente paso.\"\"\"\n",
    "\n",
    "    ## Install packages\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    def install(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package,\n",
    "                                \"--extra-index-url=https://download.pytorch.org/whl/cpu\",\n",
    "                                \"--trusted-host=download.pytorch.org\"])\n",
    "\n",
    "    install(\"torch==2.0.0+cpu\")\n",
    "    install(\"torchvision==0.15.1+cpu\")\n",
    "    #install(\"matplotlib\")    \n",
    "\n",
    "\n",
    "    ## Libraries\n",
    "    import torch\n",
    "\n",
    "    from torchvision.transforms import transforms\n",
    "    from torchvision.datasets import MNIST\n",
    "\n",
    "    ## Download MNIST DATASET\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    mnist_train  = MNIST(\".\", download=True, train=True, transform=transform)\n",
    "    mnist_test   = MNIST(\".\", download=True, train=False, transform=transform)\n",
    "\n",
    "    with open(train_dataset_path.path, \"wb\") as f:\n",
    "        torch.save(mnist_train,f)\n",
    "\n",
    "    with open(test_dataset_path.path, \"wb\") as f:\n",
    "        torch.save(mnist_test,f)\n",
    "\n",
    "    # metricas\n",
    "    metrics.log_metric(\"# Samples train dataset\", len(mnist_train))\n",
    "    metrics.log_metric(\"# Samples test dataset\",  len(mnist_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f664b37-5a41-44ed-9e82-bc502a1dd347",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "ca72efbe-f127-4441-aa57-71f9aefbb782",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10\")\n",
    "def train(train_dataset_path: Input[Dataset], \n",
    "            model_path: Output[Model], \n",
    "            model_params: Output[Markdown],\n",
    "            batch_size: int, \n",
    "            epochs: int,\n",
    "            lr: float=0.1, \n",
    "            gamma: float=0.7,\n",
    "            ):\n",
    "    \"\"\"Entrenamos un model usando pytorch. La ruta donde se guarda el modelo/artefacto se envia al guiente paso\"\"\"\n",
    "\n",
    "    ## Install packages\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    def install(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package,\n",
    "                                \"--extra-index-url=https://download.pytorch.org/whl/cpu\",\n",
    "                                \"--trusted-host=download.pytorch.org\"])\n",
    "\n",
    "    install(\"torch==2.0.0+cpu\")\n",
    "    install(\"torchvision==0.15.1+cpu\")\n",
    "    install(\"numba==0.56.4\")\n",
    "    install(\"numpy==1.23.5\")\n",
    "\n",
    "\n",
    "\n",
    "    ## Libraries\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torchvision import datasets, transforms\n",
    "    from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "    ## Read dataset\n",
    "    mnist_train = torch.load(train_dataset_path.path)\n",
    "\n",
    "    ## Net definition\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "            self.fc1 = nn.Linear(9216, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            output = F.log_softmax(x, dim=1)\n",
    "            return output\n",
    "\n",
    "\n",
    "    # Training           \n",
    "    train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size)\n",
    "    model = Net().to(\"cpu\")\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "\n",
    "\n",
    "    def _train(model, train_loader, optimizer, epoch):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(\"cpu\"), target.to(\"cpu\")\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        _train(model=model, train_loader = train_loader, optimizer=optimizer, epoch=epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "    with open(model_path.path, \"wb\") as f:\n",
    "        torch.save(model.state_dict(),f)\n",
    "\n",
    "    markdown_content = '# MNIST Model \\n\\n '\n",
    "    markdown_content+= '## Model state dict: \\n'\n",
    "\n",
    "    for param_tensor in model.state_dict():\n",
    "        markdown_content+= \"\\t\" + str(model.state_dict()[param_tensor].size()) + \"\\n\"\n",
    "\n",
    "    with open(model_params.path, 'w') as f:\n",
    "        f.write(markdown_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "211c6922-f863-4d78-88f2-3c9185d6f31d",
   "metadata": {},
   "source": [
    "### Evaluamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "36d005ad-f238-4972-b403-da880775aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=['scikit-learn'], base_image=\"python:3.10\")\n",
    "def test(test_dataset_path: Input[Dataset], \n",
    "            model_path: Input[Model],\n",
    "            cm: Output[ClassificationMetrics],\n",
    "            acc: Output[Metrics]):\n",
    "\n",
    "    from typing import Tuple, List\n",
    "\n",
    "    ## Install packages\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    def install(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package,\n",
    "                                \"--extra-index-url=https://download.pytorch.org/whl/cpu\",\n",
    "                                \"--trusted-host=download.pytorch.org\"])\n",
    "\n",
    "    install(\"torch==2.0.0+cpu\")\n",
    "    install(\"torchvision==0.15.1+cpu\")\n",
    "    install(\"numba==0.56.4\")\n",
    "    install(\"numpy==1.23.5\")\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "    ## Net definition\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "            self.fc1 = nn.Linear(9216, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            output = F.log_softmax(x, dim=1)\n",
    "            return output\n",
    "\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(model_path.path))\n",
    "    model.eval()\n",
    "\n",
    "    mnist_test = torch.load(test_dataset_path.path)\n",
    "    test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "    def _test(model, test_loader) -> Tuple[List, List, float, float]:\n",
    "\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(\"cpu\"), target.to(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "                y_true.extend(target.numpy())\n",
    "                y_pred.extend(pred.numpy())\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "\n",
    "        return y_true, y_pred, test_loss, accuracy\n",
    "\n",
    "    y_true, y_pred, test_loss, accuracy = _test(model=model, test_loader=test_loader)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "    # confusion matrix\n",
    "    cm.log_confusion_matrix(\n",
    "        [str(i) for i in range(10)],\n",
    "        confusion_matrix(y_true, y_pred).tolist() # .tolist() to convert np array to list.\n",
    "    )\n",
    "\n",
    "    acc.log_metric(\"avg_loss\",test_loss)\n",
    "    acc.log_metric(\"accuracy\",accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d72e5149-ce19-4aae-97a1-6d3acd47cb5e",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da845355-9bd2-459a-b07d-8374d034d6f1",
   "metadata": {},
   "source": [
    "### Definición\n",
    "\n",
    "A continuación vamos a:\n",
    "\n",
    "1. Definir el pipeline\n",
    "2. Compilarlo\n",
    "3. Registrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "141d79b1-6c3e-49fc-81c9-3524fca02dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el pipeline\n",
    "@dsl.pipeline(\n",
    "    name='MNIST',\n",
    "    description='Ejemplo de pipeline de entrenamiento',\n",
    ")\n",
    "def run(batch_size:int=32, epochs:int=2):\n",
    "    step_1 = download_data()\n",
    "    step_2 = train(train_dataset_path=step_1.outputs[\"train_dataset_path\"], batch_size=batch_size, epochs=epochs)\n",
    "    step_3 = test(test_dataset_path=step_1.outputs[\"test_dataset_path\"], model_path=step_2.outputs[\"model_path\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a50fd847-5d7b-4232-bb20-3aa0a0d6ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilamos el pipeline\n",
    "compiler.Compiler().compile(run, package_path='pipeline.yaml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c59f2c9-c2cf-4d9a-b569-9308f776cbda",
   "metadata": {},
   "source": [
    "A continuación registramos el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cf4a6-cecc-4287-8f91-d684d7a79292",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "namespace = client.get_user_namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd2156-5165-4a68-a4c7-35871c2f86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pipeline = client.upload_pipeline(pipeline_package_path=\"pipeline.yaml\", \n",
    "                                      pipeline_name=PIPELINE_NAME, \n",
    "                                      description=PIPELINE_DESCRIPTION, \n",
    "                                      namespace=namespace)\n",
    "except ApiException as e:\n",
    "    if \"Already exist\" in e.body:\n",
    "        print(\"Pipeline {} already exists.\".format(PIPELINE_NAME))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4565301-0e32-41d6-8891-4fddd5539a72",
   "metadata": {},
   "source": [
    "Los pipelines se pueden agrupar y versionar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3999d9b1-45c5-4e48-bd02-049879b3d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pipeline_version = client.upload_pipeline_version(\n",
    "        pipeline_package_path=\"pipeline.yaml\",\n",
    "        pipeline_version_name=PIPELINE_VERSION,\n",
    "        pipeline_id=pipeline.pipeline_id,\n",
    "        description=\"Primera version\"\n",
    "    )\n",
    "except ApiException as e:\n",
    "    if \"Already exist\" in e.body:\n",
    "        print(\"Pipeline {} with version {} already exists.\".format(pipeline.display_name, PIPELINE_VERSION))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba8d8c6d-b1c4-427a-804f-1444557c2b8b",
   "metadata": {},
   "source": [
    "### Ejecucion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32576edc-7ff2-4f51-aa09-a3eb9b5fc99c",
   "metadata": {},
   "source": [
    "Como vamos a hacer varias ejecuciones, estas se pueden agrupar por experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3851c5-5e54-4ff5-a8c9-3d7ebde73cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    experiment = client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "except:\n",
    "    experiment = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "38bde52d-5adc-41d4-8b4b-3548641b4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8945450-c740-4bc9-a016-3deacadec0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"batch_size\": 32, \"epochs\": 1}\n",
    "run_pipeline = client.run_pipeline(experiment_id=experiment.experiment_id,\n",
    "                    job_name=pipeline.display_name + \"_run_\" + time.strftime(\"%Y-%m-%d--%H:%M:%S\", time.gmtime()),\n",
    "                    params=params,\n",
    "                    pipeline_id=pipeline_version.pipeline_id,\n",
    "                    version_id=pipeline_version.pipeline_version_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93fe5b3-9eb1-45b5-b9d2-2f950ddb1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a hacer otra ejecución con otros parámetros\n",
    "params = {\"batch_size\": 64, \"epochs\": 5}\n",
    "run_pipeline = client.run_pipeline(experiment_id=experiment.experiment_id,\n",
    "                    job_name=pipeline.display_name + \"_run_\" + time.strftime(\"%Y-%m-%d--%H:%M:%S\", time.gmtime()),\n",
    "                    params=params,\n",
    "                    pipeline_id=pipeline_version.pipeline_id,\n",
    "                    version_id=pipeline_version.pipeline_version_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
